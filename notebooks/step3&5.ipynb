{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56229a22",
   "metadata": {},
   "source": [
    "# HULL TACTICAL MARKET PREDICTION - SUBMISSION NOTEBOOK\n",
    "\n",
    "This notebook introduces feature engineering to our problem and trains a LightGBM (Best performing model from step 2) on the \n",
    "preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1a4c784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: (9021, 98)\n",
      "Testing Shape: (10, 99)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# Load data\n",
    "TRAIN_PATH = \"../data/train.csv\"\n",
    "TEST_PATH  = \"../data/test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(f\"Training Shape: {train_df.shape}\\nTesting Shape: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd97b18",
   "metadata": {},
   "source": [
    "Creating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a60d70a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature parameters\n",
    "LAGS = [1, 3, 5]\n",
    "ROLL_WINDOWS = [5, 10]\n",
    "\n",
    "def create_features(df, base_cols):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # ----- LAGS -----\n",
    "    for col in base_cols:\n",
    "        for lag in LAGS:\n",
    "            df[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
    "\n",
    "    # ----- ROLLING FEATURES -----\n",
    "    for col in base_cols:\n",
    "        for w in ROLL_WINDOWS:\n",
    "            df[f\"{col}_roll_mean_{w}\"] = df[col].rolling(w).mean()\n",
    "            df[f\"{col}_roll_std_{w}\"]  = df[col].rolling(w).std()\n",
    "\n",
    "    # Simple fills\n",
    "    df = df.ffill().bfill().fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b951430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Final selected features (quota features from EDA)\n",
    "# ============================================================\n",
    "\n",
    "FEATURE_COLS = ['E19', 'E3', 'E2', 'E4', 'E13', 'S2', 'S5', 'S6', 'V3', 'V13', 'V7', 'V5', 'I6', 'I2', 'M4', 'M3', 'M12', 'P11', 'P5', 'P10']\n",
    "\n",
    "TARGET = \"market_forward_excess_returns\"\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a08089",
   "metadata": {},
   "source": [
    "Defining evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69d1cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle official scoring function\n",
    "\n",
    "MIN_INVESTMENT = 0\n",
    "MAX_INVESTMENT = 2\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name=None) -> float:\n",
    "\n",
    "    if not pd.api.types.is_numeric_dtype(submission['prediction']):\n",
    "        raise ParticipantVisibleError('Predictions must be numeric')\n",
    "\n",
    "    solution = solution.copy()\n",
    "    solution['position'] = submission['prediction']\n",
    "\n",
    "    strategy_returns = (\n",
    "        solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "        solution['position'] * solution['forward_returns']\n",
    "    )\n",
    "\n",
    "    strategy_excess_returns = strategy_returns - solution['risk_free_rate']\n",
    "    strat_cum = (1 + strategy_excess_returns).prod()\n",
    "    strat_mean = strat_cum ** (1/len(solution)) - 1\n",
    "    strategy_std = strategy_returns.std()\n",
    "\n",
    "    trading_days = 252\n",
    "    if strategy_std == 0:\n",
    "        return 0.0\n",
    "\n",
    "    sharpe = strat_mean / strategy_std * np.sqrt(trading_days)\n",
    "    strategy_vol = float(strategy_std * np.sqrt(trading_days) * 100)\n",
    "\n",
    "    market_excess = solution['forward_returns'] - solution['risk_free_rate']\n",
    "    market_cum = (1 + market_excess).prod()\n",
    "    market_mean = market_cum ** (1/len(solution)) - 1\n",
    "    market_std = solution['forward_returns'].std()\n",
    "    market_vol = float(market_std * np.sqrt(trading_days) * 100)\n",
    "\n",
    "    excess_vol = max(0, strategy_vol/market_vol - 1.2)\n",
    "    vol_penalty = 1 + excess_vol\n",
    "\n",
    "    return_gap = max(0, (market_mean - strat_mean) * 100 * trading_days)\n",
    "    return_penalty = 1 + (return_gap**2)/100\n",
    "\n",
    "    adjusted = sharpe / (vol_penalty * return_penalty)\n",
    "    return float(min(adjusted, 1_000_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3a907d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction → Allocation + Local Kaggle metric\n",
    "def returns_to_allocation(pred):\n",
    "    if pred <= 0:\n",
    "        return 0.0\n",
    "    if pred >= 0.01:\n",
    "        return 2.0\n",
    "    return 2.0 * (pred / 0.01)\n",
    "\n",
    "def kaggle_local_score(model, X_valid, df_valid):\n",
    "    raw_preds = model.predict(X_valid)\n",
    "    raw_preds = np.clip(raw_preds, -0.05, 0.05)\n",
    "\n",
    "    allocs = np.array([returns_to_allocation(p) for p in raw_preds])\n",
    "\n",
    "    submission = pd.DataFrame({\"prediction\": allocs})\n",
    "    solution = pd.DataFrame({\n",
    "        \"forward_returns\": df_valid[\"forward_returns\"].values,\n",
    "        \"risk_free_rate\": df_valid[\"risk_free_rate\"].values\n",
    "    })\n",
    "\n",
    "    return score(solution, submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a36544",
   "metadata": {},
   "source": [
    "Model Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43184343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LGBM params: {'n_estimators': 800, 'learning_rate': 0.02, 'num_leaves': 120, 'subsample': 0.7, 'colsample_bytree': 0.7, 'max_depth': -1, 'min_child_samples': 20, 'reg_alpha': 0.0, 'reg_lambda': 0.0}\n",
      "→ Score: 0.35538956176589326\n",
      "Testing LGBM params: {'n_estimators': 1000, 'learning_rate': 0.015, 'num_leaves': 150, 'subsample': 0.8, 'colsample_bytree': 0.8, 'max_depth': -1, 'min_child_samples': 30, 'reg_alpha': 0.0, 'reg_lambda': 0.0}\n",
      "→ Score: 0.3017576083563382\n",
      "Testing LGBM params: {'n_estimators': 600, 'learning_rate': 0.03, 'num_leaves': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'max_depth': 10, 'min_child_samples': 10, 'reg_alpha': 0.0, 'reg_lambda': 0.0}\n",
      "→ Score: 0.5181689055683574\n",
      "Testing LGBM params: {'n_estimators': 900, 'learning_rate': 0.012, 'num_leaves': 80, 'subsample': 0.9, 'colsample_bytree': 0.9, 'max_depth': 8, 'min_child_samples': 40, 'reg_alpha': 0.1, 'reg_lambda': 0.1}\n",
      "→ Score: 0.3088060256235065\n",
      "Testing LGBM params: {'n_estimators': 1200, 'learning_rate': 0.01, 'num_leaves': 160, 'subsample': 1.0, 'colsample_bytree': 1.0, 'max_depth': -1, 'min_child_samples': 50, 'reg_alpha': 0.3, 'reg_lambda': 0.5}\n",
      "→ Score: 0.34324092573611387\n",
      "\n",
      "BEST LGBM SCORE: 0.5181689055683574\n",
      "BEST LGBM PARAMS: {'n_estimators': 600, 'learning_rate': 0.03, 'num_leaves': 100, 'subsample': 0.6, 'colsample_bytree': 0.6, 'max_depth': 10, 'min_child_samples': 10, 'reg_alpha': 0.0, 'reg_lambda': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# LightGBM Hyperparameter Search (FAST GRID)\n",
    "\n",
    "def evaluate_lgbm_params(params):\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    scores = []\n",
    "    FE_FEATURE_COLS = None\n",
    "\n",
    "    for tr_idx, va_idx in tscv.split(train_df):\n",
    "\n",
    "        df_tr = train_df.iloc[tr_idx].copy()\n",
    "        df_va = train_df.iloc[va_idx].copy()\n",
    "\n",
    "        # FE SUR LE TRAIN SEULEMENT\n",
    "        df_tr_fe = create_features(df_tr, FEATURE_COLS)\n",
    "\n",
    "        # FE POUR LA VALIDATION = train + valid, puis on coupe\n",
    "        df_combined = pd.concat([df_tr, df_va], ignore_index=True)\n",
    "        df_combined_fe = create_features(df_combined, FEATURE_COLS)\n",
    "        df_va_fe = df_combined_fe.iloc[len(df_tr):]\n",
    "\n",
    "        # On définit les features FE une seule fois\n",
    "        if FE_FEATURE_COLS is None:\n",
    "            FE_FEATURE_COLS = [\n",
    "                c for c in df_tr_fe.columns\n",
    "                if c not in [TARGET, 'forward_returns', 'risk_free_rate']\n",
    "            ]\n",
    "\n",
    "        Xtr = df_tr_fe[FE_FEATURE_COLS]\n",
    "        ytr = df_tr_fe[TARGET]\n",
    "\n",
    "        Xva = df_va_fe[FE_FEATURE_COLS]\n",
    "        yva = df_va_fe[TARGET]\n",
    "\n",
    "        model = lgb.LGBMRegressor(**params, random_state=42, verbose=-1)\n",
    "        model.fit(Xtr, ytr)\n",
    "\n",
    "        score = kaggle_local_score(model, Xva, df_va)\n",
    "        scores.append(score)\n",
    "\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "\n",
    "lgbm_param_grid = [\n",
    "\n",
    "    # --- Around the previous best found config ---\n",
    "    {\"n_estimators\": 800, \"learning_rate\": 0.02, \"num_leaves\": 120, \"subsample\": 0.7, \"colsample_bytree\": 0.7,\n",
    "     \"max_depth\": -1, \"min_child_samples\": 20, \"reg_alpha\": 0.0, \"reg_lambda\": 0.0},\n",
    "\n",
    "    {\"n_estimators\": 1000, \"learning_rate\": 0.015, \"num_leaves\": 150, \"subsample\": 0.8, \"colsample_bytree\": 0.8,\n",
    "     \"max_depth\": -1, \"min_child_samples\": 30, \"reg_alpha\": 0.0, \"reg_lambda\": 0.0},\n",
    "\n",
    "    {\"n_estimators\": 600, \"learning_rate\": 0.03, \"num_leaves\": 100, \"subsample\": 0.6, \"colsample_bytree\": 0.6,\n",
    "     \"max_depth\": 10, \"min_child_samples\": 10, \"reg_alpha\": 0.0, \"reg_lambda\": 0.0},\n",
    "\n",
    "    # --- More robust / regularized versions ---\n",
    "    {\"n_estimators\": 900, \"learning_rate\": 0.012, \"num_leaves\": 80, \"subsample\": 0.9, \"colsample_bytree\": 0.9,\n",
    "     \"max_depth\": 8, \"min_child_samples\": 40, \"reg_alpha\": 0.1, \"reg_lambda\": 0.1},\n",
    "\n",
    "    {\"n_estimators\": 1200, \"learning_rate\": 0.01, \"num_leaves\": 160, \"subsample\": 1.0, \"colsample_bytree\": 1.0,\n",
    "     \"max_depth\": -1, \"min_child_samples\": 50, \"reg_alpha\": 0.3, \"reg_lambda\": 0.5},\n",
    "]\n",
    "\n",
    "best_lgbm_score = -999\n",
    "best_lgbm_params = None\n",
    "\n",
    "for params in lgbm_param_grid:\n",
    "    print(\"Testing LGBM params:\", params)\n",
    "    score_mean = evaluate_lgbm_params(params)\n",
    "    print(\"→ Score:\", score_mean)\n",
    "\n",
    "    if score_mean > best_lgbm_score:\n",
    "        best_lgbm_score = score_mean\n",
    "        best_lgbm_params = params\n",
    "\n",
    "print(\"\\nBEST LGBM SCORE:\", best_lgbm_score)\n",
    "print(\"BEST LGBM PARAMS:\", best_lgbm_params)\n",
    "\n",
    "\n",
    "# Define model\n",
    "lgbm_model = lgb.LGBMRegressor(**best_lgbm_params, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314bb46",
   "metadata": {},
   "source": [
    "Local Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af2b08e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of FE features: 235\n",
      "LGBM fold score = 0.5851272498288512\n",
      "LGBM fold score = 0.5418125232149132\n",
      "LGBM fold score = 0.42386553209091005\n",
      "LGBM fold score = 0.8406712293333722\n",
      "LGBM fold score = 0.19936799337374\n",
      "\n",
      "MEAN SCORE (LGBM): 0.5181689055683574\n"
     ]
    }
   ],
   "source": [
    "# Local model evaluation using Kaggle metric\n",
    "\n",
    "models = {\n",
    "    \"LGBM\": lgbm_model\n",
    "}\n",
    "\n",
    "scores = {\"LGBM\": []}\n",
    "FE_FEATURE_COLS = None  # sera défini au premier fold\n",
    "\n",
    "for train_idx, valid_idx in tscv.split(train_df):\n",
    "\n",
    "    # 1) Train & valid split for this fold\n",
    "    df_train_fold = train_df.iloc[train_idx].copy()\n",
    "    df_valid_fold = train_df.iloc[valid_idx].copy()\n",
    "\n",
    "    # 2) Apply FE on train only\n",
    "    df_train_fe = create_features(df_train_fold, FEATURE_COLS)\n",
    "\n",
    "    # Apply FE on train+valid to avoid leakage in rolling features\n",
    "    df_combined = pd.concat([df_train_fold, df_valid_fold], ignore_index=True)\n",
    "    df_combined_fe = create_features(df_combined, FEATURE_COLS)\n",
    "    df_valid_fe = df_combined_fe.iloc[len(df_train_fold):].copy()\n",
    "\n",
    "    # 3) Determine FE feature columns once\n",
    "    if FE_FEATURE_COLS is None:\n",
    "        FE_FEATURE_COLS = [\n",
    "            c for c in df_train_fe.columns\n",
    "            if c not in [TARGET, \"forward_returns\", \"risk_free_rate\"]\n",
    "        ]\n",
    "        print(\"Number of FE features:\", len(FE_FEATURE_COLS))\n",
    "\n",
    "    # Extract matrices\n",
    "    Xtr = df_train_fe[FE_FEATURE_COLS]\n",
    "    ytr = df_train_fe[TARGET]\n",
    "\n",
    "    Xva = df_valid_fe[FE_FEATURE_COLS]\n",
    "    yva = df_valid_fe[TARGET]\n",
    "\n",
    "    df_va_solution = df_valid_fold  # needed for Kaggle metric\n",
    "\n",
    "    # 4) Evaluate the LightGBM model\n",
    "    model = lgbm_model\n",
    "    model.fit(Xtr, ytr)\n",
    "\n",
    "    fold_score = kaggle_local_score(model, Xva, df_va_solution)\n",
    "    scores[\"LGBM\"].append(fold_score)\n",
    "\n",
    "    print(f\"LGBM fold score = {fold_score}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nMEAN SCORE (LGBM):\", np.mean(scores[\"LGBM\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea70cf8f",
   "metadata": {},
   "source": [
    "Final Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfd77146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final LightGBM model trained on full FE dataset.\n",
      "Training shape: (9021, 235)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Train final LightGBM model on full FE dataset\n",
    "# ============================================================\n",
    "\n",
    "# 1) Apply FE to the full training set\n",
    "train_fe_full = create_features(train_df, FEATURE_COLS)\n",
    "\n",
    "# 2) Use the FE columns discovered earlier (from cross-validation)\n",
    "X_full = train_fe_full[FE_FEATURE_COLS]\n",
    "y_full = train_fe_full[TARGET]\n",
    "\n",
    "# 3) Train the final LightGBM model\n",
    "final_lgbm_model = lgb.LGBMRegressor(**best_lgbm_params, random_state=42, verbose=-1)\n",
    "final_lgbm_model.fit(X_full, y_full)\n",
    "\n",
    "print(\"Final LightGBM model trained on full FE dataset.\")\n",
    "print(\"Training shape:\", X_full.shape)\n",
    "\n",
    "# For later inference\n",
    "best_model = final_lgbm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480084f1",
   "metadata": {},
   "source": [
    "Kaggle submission specifics (IGNORE FOR LOCAL RUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "591ef8f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kaggle_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36120\\122809102.py\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Start server\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0minference_server\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkaggle_eval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDefaultInferenceServer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"KAGGLE_IS_COMPETITION_RERUN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kaggle_eval' is not defined"
     ]
    }
   ],
   "source": [
    "# Inference API — Kaggle submission (not needed for local version)\n",
    "\n",
    "history_df = None\n",
    "model_loaded = True\n",
    "\n",
    "def to_pandas(df):\n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        return df.to_pandas()\n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    global history_df, best_model\n",
    "\n",
    "    test_pd = to_pandas(test)\n",
    "\n",
    "    if history_df is None:\n",
    "        history_df = test_pd.copy()\n",
    "    else:\n",
    "        history_df = pd.concat([history_df, test_pd], ignore_index=True)\n",
    "\n",
    "    window = history_df.copy()  # ou un tail(...) si tu veux limiter la taille\n",
    "    window_fe = create_features(window, FEATURE_COLS)\n",
    "    \n",
    "    X_last = window_fe[FE_FEATURE_COLS].iloc[-1:]\n",
    "    raw_pred = float(best_model.predict(X_last)[0])\n",
    "    raw_pred = float(np.clip(raw_pred, -0.05, 0.05))\n",
    "\n",
    "    alloc = returns_to_allocation(raw_pred)\n",
    "    return float(alloc)\n",
    "\n",
    "# Start server\n",
    "inference_server = kaggle_eval.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\"/kaggle/input/hull-tactical-market-prediction/\",)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ee488",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
