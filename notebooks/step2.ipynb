{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4da5b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dddd7713",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import lightgbm as lgb\n",
    "LGBM_AVAILABLE = True\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "LSTM_AVAILABLE = True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56708c",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe9e0d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get project directory\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_DIR = NOTEBOOK_DIR.parent\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'train_data_path': PROJECT_DIR / 'data' / 'train.csv',\n",
    "    'test_data_path': PROJECT_DIR / 'data' / 'test.csv',\n",
    "    'artifacts_dir': PROJECT_DIR / 'artifacts',\n",
    "    'results_dir': PROJECT_DIR / 'results',\n",
    "    'target_column': 'forward_returns',\n",
    "    'prediction_bounds': (0.0, 2.0),\n",
    "    'max_volatility_ratio': 1.2,  # 120% of benchmark\n",
    "    'n_splits': 5,  # For time-series cross-validation\n",
    "}\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "CONFIG['results_dir'].mkdir(exist_ok=True)\n",
    "\n",
    "# Feature list (same as Step 1)\n",
    "FEATURE_COLS = [\n",
    "    'V7', 'V13', 'E3', 'M4', 'P11', 'S2', 'I7', 'P2', 'P6', 'E17',\n",
    "    'M12', 'M11', 'E2', 'I2', 'E9', 'P8', 'P5', 'I5', 'I9', 'V12'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea871c13",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f138b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data: (9021, 98)\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_df = pd.read_csv(CONFIG['train_data_path'])\n",
    "print(f\"Loaded training data: {train_df.shape}\")\n",
    "# Safely detect a date column before accessing it to avoid KeyError\n",
    "date_col = next((c for c in ['date','timestamp','datetime','time','Date','DATE'] if c in train_df.columns), None)\n",
    "if date_col:\n",
    "    date_min = pd.to_datetime(train_df[date_col], errors='coerce').min()\n",
    "    date_max = pd.to_datetime(train_df[date_col], errors='coerce').max()\n",
    "    if pd.notna(date_min) and pd.notna(date_max):\n",
    "        print(f\"Date range ({date_col}): {date_min} to {date_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5e99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned: 9,021 samples\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "X = train_df[FEATURE_COLS].copy()\n",
    "y = train_df[CONFIG['target_column']].copy()\n",
    "# Safely build a `dates` series from available date-like columns\n",
    "date_col = next((c for c in ['date','timestamp','datetime','time','Date','DATE'] if c in train_df.columns), None)\n",
    "if date_col:\n",
    "    dates = pd.to_datetime(train_df[date_col], errors='coerce')\n",
    "else:\n",
    "    dates = pd.Series(pd.NaT, index=train_df.index)\n",
    "\n",
    "# Handle missing values\n",
    "X = X.ffill().bfill()\n",
    "\n",
    "# Remove rows with NaN\n",
    "valid_idx = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "X = X[valid_idx]\n",
    "y = y[valid_idx]\n",
    "dates = dates[valid_idx]\n",
    "\n",
    "print(f\"Data cleaned: {len(X):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54086327",
   "metadata": {},
   "source": [
    "## Time-Series Cross-Validation Setup\n",
    "\n",
    "Using **Walk-Forward** validation to respect temporal ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2cbab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-series cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=CONFIG['n_splits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e9d3ca",
   "metadata": {},
   "source": [
    "## Model 1: Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68fc02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL 1: RANDOM FOREST\n",
      "======================================================================\n",
      "\n",
      "Fold 1/5\n",
      "  MSE: 0.000148\n",
      "  MAE: 0.009316\n",
      "  R²: -0.014001\n",
      "\n",
      "Fold 2/5\n",
      "  MSE: 0.000105\n",
      "  MAE: 0.007374\n",
      "  R²: -0.080307\n",
      "\n",
      "Fold 3/5\n",
      "  MSE: 0.000105\n",
      "  MAE: 0.007374\n",
      "  R²: -0.080307\n",
      "\n",
      "Fold 3/5\n",
      "  MSE: 0.000177\n",
      "  MAE: 0.009373\n",
      "  R²: -0.018052\n",
      "\n",
      "Fold 4/5\n",
      "  MSE: 0.000177\n",
      "  MAE: 0.009373\n",
      "  R²: -0.018052\n",
      "\n",
      "Fold 4/5\n",
      "  MSE: 0.000069\n",
      "  MAE: 0.005733\n",
      "  R²: -0.016737\n",
      "\n",
      "Fold 5/5\n",
      "  MSE: 0.000069\n",
      "  MAE: 0.005733\n",
      "  R²: -0.016737\n",
      "\n",
      "Fold 5/5\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 1: RANDOM FOREST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rf_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n",
    "    print(f\"\\nFold {fold}/{CONFIG['n_splits']}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_fold)\n",
    "    X_val_scaled = scaler.transform(X_val_fold)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_model.fit(X_train_scaled, y_train_fold)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = rf_model.predict(X_val_scaled)\n",
    "    \n",
    "    # Clip predictions to valid range\n",
    "    y_pred = np.clip(y_pred, CONFIG['prediction_bounds'][0], CONFIG['prediction_bounds'][1])\n",
    "    \n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    mae = mean_absolute_error(y_val_fold, y_pred)\n",
    "    r2 = r2_score(y_val_fold, y_pred)\n",
    "    \n",
    "    rf_results.append({\n",
    "        'fold': fold,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    })\n",
    "    \n",
    "    print(f\"  MSE: {mse:.6f}\")\n",
    "    print(f\"  MAE: {mae:.6f}\")\n",
    "    print(f\"  R²: {r2:.6f}\")\n",
    "\n",
    "# Average results\n",
    "rf_results_df = pd.DataFrame(rf_results)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"RANDOM FOREST - AVERAGE RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"MSE: {rf_results_df['mse'].mean():.6f} ± {rf_results_df['mse'].std():.6f}\")\n",
    "print(f\"MAE: {rf_results_df['mae'].mean():.6f} ± {rf_results_df['mae'].std():.6f}\")\n",
    "print(f\"R²:  {rf_results_df['r2'].mean():.6f} ± {rf_results_df['r2'].std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e62f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final Random Forest on all data\n",
    "print(\"\\nTraining final Random Forest on all data...\")\n",
    "\n",
    "scaler_rf = StandardScaler()\n",
    "X_scaled = scaler_rf.fit_transform(X)\n",
    "\n",
    "rf_final = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_final.fit(X_scaled, y)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(rf_final, CONFIG['artifacts_dir'] / 'rf_model.joblib')\n",
    "joblib.dump(scaler_rf, CONFIG['artifacts_dir'] / 'rf_scaler.joblib')\n",
    "\n",
    "print(\"Random Forest model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6cdef6",
   "metadata": {},
   "source": [
    "## Model 2: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3de6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LGBM_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL 2: LIGHTGBM\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    lgbm_results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n",
    "        print(f\"\\nFold {fold}/{CONFIG['n_splits']}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Train LightGBM\n",
    "        lgbm_model = lgb.LGBMRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            num_leaves=31,\n",
    "            min_child_samples=20,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "        lgbm_model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)]\n",
    "        )\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = lgbm_model.predict(X_val_fold)\n",
    "        y_pred = np.clip(y_pred, CONFIG['prediction_bounds'][0], CONFIG['prediction_bounds'][1])\n",
    "        \n",
    "        # Evaluate\n",
    "        mse = mean_squared_error(y_val_fold, y_pred)\n",
    "        mae = mean_absolute_error(y_val_fold, y_pred)\n",
    "        r2 = r2_score(y_val_fold, y_pred)\n",
    "        \n",
    "        lgbm_results.append({\n",
    "            'fold': fold,\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'r2': r2\n",
    "        })\n",
    "        \n",
    "        print(f\"  MSE: {mse:.6f}\")\n",
    "        print(f\"  MAE: {mae:.6f}\")\n",
    "        print(f\"  R²: {r2:.6f}\")\n",
    "    \n",
    "    # Average results\n",
    "    lgbm_results_df = pd.DataFrame(lgbm_results)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"LIGHTGBM - AVERAGE RESULTS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"MSE: {lgbm_results_df['mse'].mean():.6f} ± {lgbm_results_df['mse'].std():.6f}\")\n",
    "    print(f\"MAE: {lgbm_results_df['mae'].mean():.6f} ± {lgbm_results_df['mae'].std():.6f}\")\n",
    "    print(f\"R²:  {lgbm_results_df['r2'].mean():.6f} ± {lgbm_results_df['r2'].std():.6f}\")\n",
    "else:\n",
    "    print(\"Skipping LightGBM (not installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d04c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LGBM_AVAILABLE:\n",
    "    # Train final LightGBM on all data\n",
    "    print(\"\\nTraining final LightGBM on all data...\")\n",
    "    \n",
    "    lgbm_final = lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgbm_final.fit(X, y)\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(lgbm_final, CONFIG['artifacts_dir'] / 'lgbm_model.joblib')\n",
    "    \n",
    "    print(\"LightGBM model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa9961d",
   "metadata": {},
   "source": [
    "## Model 3: LSTM (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280baffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LSTM_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL 3: LSTM\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Prepare data for LSTM (need 3D input: samples, timesteps, features)\n",
    "    # We'll use a simple approach: each sample is a single timestep\n",
    "    \n",
    "    lstm_results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n",
    "        print(f\"\\nFold {fold}/{CONFIG['n_splits']}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_fold)\n",
    "        X_val_scaled = scaler.transform(X_val_fold)\n",
    "        \n",
    "        # Reshape for LSTM (samples, timesteps, features)\n",
    "        X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "        X_val_lstm = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))\n",
    "        \n",
    "        # Build LSTM model\n",
    "        model = Sequential([\n",
    "            LSTM(50, activation='relu', input_shape=(1, len(FEATURE_COLS)), return_sequences=True),\n",
    "            Dropout(0.2),\n",
    "            LSTM(25, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "        \n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            X_train_lstm, y_train_fold,\n",
    "            validation_data=(X_val_lstm, y_val_fold),\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_val_lstm, verbose=0).flatten()\n",
    "        y_pred = np.clip(y_pred, CONFIG['prediction_bounds'][0], CONFIG['prediction_bounds'][1])\n",
    "        \n",
    "        # Evaluate\n",
    "        mse = mean_squared_error(y_val_fold, y_pred)\n",
    "        mae = mean_absolute_error(y_val_fold, y_pred)\n",
    "        r2 = r2_score(y_val_fold, y_pred)\n",
    "        \n",
    "        lstm_results.append({\n",
    "            'fold': fold,\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'r2': r2\n",
    "        })\n",
    "        \n",
    "        print(f\"  MSE: {mse:.6f}\")\n",
    "        print(f\"  MAE: {mae:.6f}\")\n",
    "        print(f\"  R²: {r2:.6f}\")\n",
    "        print(f\"  Epochs trained: {len(history.history['loss'])}\")\n",
    "    \n",
    "    # Average results\n",
    "    lstm_results_df = pd.DataFrame(lstm_results)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"LSTM - AVERAGE RESULTS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"MSE: {lstm_results_df['mse'].mean():.6f} ± {lstm_results_df['mse'].std():.6f}\")\n",
    "    print(f\"MAE: {lstm_results_df['mae'].mean():.6f} ± {lstm_results_df['mae'].std():.6f}\")\n",
    "    print(f\"R²:  {lstm_results_df['r2'].mean():.6f} ± {lstm_results_df['r2'].std():.6f}\")\n",
    "else:\n",
    "    print(\"Skipping LSTM (TensorFlow not installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7bb494",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LSTM_AVAILABLE:\n",
    "    # Train final LSTM on all data\n",
    "    print(\"\\nTraining final LSTM on all data...\")\n",
    "    \n",
    "    scaler_lstm = StandardScaler()\n",
    "    X_scaled = scaler_lstm.fit_transform(X)\n",
    "    X_lstm = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "    \n",
    "    lstm_final = Sequential([\n",
    "        LSTM(50, activation='relu', input_shape=(1, len(FEATURE_COLS)), return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(25, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    lstm_final.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "    \n",
    "    lstm_final.fit(\n",
    "        X_lstm, y,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    lstm_final.save(CONFIG['artifacts_dir'] / 'lstm_model.h5')\n",
    "    joblib.dump(scaler_lstm, CONFIG['artifacts_dir'] / 'lstm_scaler.joblib')\n",
    "    \n",
    "    print(\"LSTM model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1755fe44",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f765bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison = []\n",
    "\n",
    "# Random Forest\n",
    "comparison.append({\n",
    "    'Model': 'Random Forest',\n",
    "    'MSE': f\"{rf_results_df['mse'].mean():.6f} ± {rf_results_df['mse'].std():.6f}\",\n",
    "    'MAE': f\"{rf_results_df['mae'].mean():.6f} ± {rf_results_df['mae'].std():.6f}\",\n",
    "    'R²': f\"{rf_results_df['r2'].mean():.6f} ± {rf_results_df['r2'].std():.6f}\"\n",
    "})\n",
    "\n",
    "# LightGBM\n",
    "if LGBM_AVAILABLE:\n",
    "    comparison.append({\n",
    "        'Model': 'LightGBM',\n",
    "        'MSE': f\"{lgbm_results_df['mse'].mean():.6f} ± {lgbm_results_df['mse'].std():.6f}\",\n",
    "        'MAE': f\"{lgbm_results_df['mae'].mean():.6f} ± {lgbm_results_df['mae'].std():.6f}\",\n",
    "        'R²': f\"{lgbm_results_df['r2'].mean():.6f} ± {lgbm_results_df['r2'].std():.6f}\"\n",
    "    })\n",
    "\n",
    "# LSTM\n",
    "if LSTM_AVAILABLE:\n",
    "    comparison.append({\n",
    "        'Model': 'LSTM',\n",
    "        'MSE': f\"{lstm_results_df['mse'].mean():.6f} ± {lstm_results_df['mse'].std():.6f}\",\n",
    "        'MAE': f\"{lstm_results_df['mae'].mean():.6f} ± {lstm_results_df['mae'].std():.6f}\",\n",
    "        'R²': f\"{lstm_results_df['r2'].mean():.6f} ± {lstm_results_df['r2'].std():.6f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv(CONFIG['results_dir'] / 'model_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44857b85",
   "metadata": {},
   "source": [
    "## Volatility Check\n",
    "\n",
    "Ensure predictions don't exceed 120% of benchmark volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d3395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate volatility of predictions vs target\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VOLATILITY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get predictions from best model (Random Forest for now)\n",
    "X_scaled_all = scaler_rf.transform(X)\n",
    "predictions = rf_final.predict(X_scaled_all)\n",
    "predictions = np.clip(predictions, CONFIG['prediction_bounds'][0], CONFIG['prediction_bounds'][1])\n",
    "\n",
    "# Calculate volatilities\n",
    "target_vol = y.std()\n",
    "pred_vol = predictions.std()\n",
    "vol_ratio = pred_vol / target_vol\n",
    "\n",
    "print(f\"Target (benchmark) volatility: {target_vol:.6f}\")\n",
    "print(f\"Prediction volatility: {pred_vol:.6f}\")\n",
    "print(f\"Volatility ratio: {vol_ratio:.2%}\")\n",
    "print(f\"Max allowed ratio: {CONFIG['max_volatility_ratio']:.0%}\")\n",
    "\n",
    "if vol_ratio <= CONFIG['max_volatility_ratio']:\n",
    "    print(f\"Volatility constraint satisfied\")\n",
    "else:\n",
    "    print(f\"Warning: Volatility exceeds {CONFIG['max_volatility_ratio']:.0%} threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c71b97",
   "metadata": {},
   "source": [
    "## Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517679a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': FEATURE_COLS,\n",
    "    'importance': rf_final.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 10 MOST IMPORTANT FEATURES (Random Forest)\")\n",
    "print(\"=\"*70)\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['feature'][:10], feature_importance['importance'][:10])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 10 Feature Importance (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['results_dir'] / 'feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
