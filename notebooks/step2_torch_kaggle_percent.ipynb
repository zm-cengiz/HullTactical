{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and environment detection\n",
    "import os\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# ============================================================================\n",
    "# GLOBAL CONFIGURATION CONSTANTS\n",
    "# ============================================================================\n",
    "MISSING_DATA_THRESHOLD = 0.10  # Drop features with >10% missing values\n",
    "# ============================================================================\n",
    "\n",
    "IN_KAGGLE = Path('/kaggle/input').exists()\n",
    "LGBM_AVAILABLE = importlib.util.find_spec('lightgbm') is not None\n",
    "TORCH_AVAILABLE = importlib.util.find_spec('torch') is not None\n",
    "\n",
    "if LGBM_AVAILABLE:\n",
    "    import lightgbm as lgb\n",
    "\n",
    "if TORCH_AVAILABLE:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f'IN_KAGGLE={IN_KAGGLE}, LGBM_AVAILABLE={LGBM_AVAILABLE}, TORCH_AVAILABLE={TORCH_AVAILABLE}')\n",
    "print(f'MISSING_DATA_THRESHOLD={MISSING_DATA_THRESHOLD:.0%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1768da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and paths\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_DIR = NOTEBOOK_DIR.parent\n",
    "IN_KAGGLE = Path('/kaggle/input').exists()\n",
    "\n",
    "def resolve_kaggle_path(preferred: Path, default_filename: str) -> Path:\n",
    "    if not IN_KAGGLE:\n",
    "        return preferred\n",
    "    env_key = default_filename.upper().replace('.CSV', '') + '_PATH'\n",
    "    env_val = os.environ.get(env_key)\n",
    "    if env_val and Path(env_val).exists():\n",
    "        return Path(env_val)\n",
    "    candidates = list(Path('/kaggle/input').glob(f'**/{default_filename}'))\n",
    "    if candidates:\n",
    "        return candidates[0]\n",
    "    any_csv = list(Path('/kaggle/input').glob('**/*.csv'))\n",
    "    if any_csv:\n",
    "        print(f'Warning: Could not find {default_filename}; using first CSV found: {any_csv[0]}')\n",
    "        return any_csv[0]\n",
    "    print(f'Warning: No CSV found under /kaggle/input; attempting preferred path: {preferred}')\n",
    "    return preferred\n",
    "\n",
    "# Directories for artifacts/results \n",
    "if IN_KAGGLE:\n",
    "    ARTIFACTS_DIR = Path('/kaggle/working/artifacts')\n",
    "    RESULTS_DIR = Path('/kaggle/working/results')\n",
    "else:\n",
    "    ARTIFACTS_DIR = PROJECT_DIR / 'artifacts'\n",
    "    RESULTS_DIR = PROJECT_DIR / 'results'\n",
    "\n",
    "CONFIG = {\n",
    "    'train_data_path': resolve_kaggle_path(PROJECT_DIR / 'data' / 'train.csv', 'train.csv'),\n",
    "    'test_data_path': resolve_kaggle_path(PROJECT_DIR / 'data' / 'test.csv', 'test.csv'),\n",
    "    'artifacts_dir': ARTIFACTS_DIR,\n",
    "    'results_dir': RESULTS_DIR,\n",
    "    'target_column': 'forward_returns',\n",
    "    'prediction_bounds': (0.0, 2.0),\n",
    "    'max_volatility_ratio': 1.2,\n",
    "    'n_splits': 5,\n",
    "    'missing_threshold': MISSING_DATA_THRESHOLD,  # Use global constant\n",
    "}\n",
    "CONFIG['results_dir'].mkdir(parents=True, exist_ok=True)\n",
    "CONFIG['artifacts_dir'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# FEATURE_COLS will be inferred dynamically in the data loading cell based on missingness\n",
    "FEATURE_COLS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd42fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities: volatility enforcement\n",
    "import numpy as _np\n",
    "\n",
    "def enforce_volatility(preds, target_std, max_ratio=1.2, clip_bounds=(0.0, 2.0)):\n",
    "    \"\"\"Scale predictions around their mean so std(preds) <= max_ratio * target_std.\n",
    "    Returns a numpy array; optionally clips to bounds.\n",
    "    \"\"\"\n",
    "    arr = _np.asarray(preds, dtype=float).reshape(-1)\n",
    "    pred_std = float(arr.std())\n",
    "    tgt_std = float(target_std)\n",
    "    if pred_std > 0 and tgt_std > 0:\n",
    "        scale = min(1.0, (max_ratio * tgt_std) / (pred_std + 1e-12))\n",
    "        mean = float(arr.mean())\n",
    "        arr = mean + scale * (arr - mean)\n",
    "    if clip_bounds is not None:\n",
    "        arr = _np.clip(arr, clip_bounds[0], clip_bounds[1])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1562a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Prepare Data\n",
    "train_df = pd.read_csv(CONFIG['train_data_path'])\n",
    "print(f\"Loaded training data: {train_df.shape} from {CONFIG['train_data_path']}\")\n",
    "\n",
    "# Optional date range info\n",
    "_date_col = next((c for c in ['date','timestamp','datetime','time','Date','DATE'] if c in train_df.columns), None)\n",
    "if _date_col:\n",
    "    _dmn = pd.to_datetime(train_df[_date_col], errors='coerce').min()\n",
    "    _dmx = pd.to_datetime(train_df[_date_col], errors='coerce').max()\n",
    "    if pd.notna(_dmn) and pd.notna(_dmx):\n",
    "        print(f\"Date range ({_date_col}): {_dmn} to {_dmx}\")\n",
    "\n",
    "# Try to load public test to ensure feature intersection (avoids train-only/leaky columns)\n",
    "_test_cols = None\n",
    "try:\n",
    "    if Path(CONFIG['test_data_path']).exists():\n",
    "        _test_df_head = pd.read_csv(CONFIG['test_data_path'], nrows=5)\n",
    "        _test_cols = set(_test_df_head.columns)\n",
    "except Exception:\n",
    "    _test_cols = None\n",
    "\n",
    "# Infer features by missingness (drop columns with > missing_threshold) and by presence in test\n",
    "missing_thr = CONFIG.get('missing_threshold', 0.30)\n",
    "# Candidate features: numeric columns excluding the target and obvious meta columns\n",
    "meta_cols = set([CONFIG['target_column']])\n",
    "meta_cols |= set([c for c in ['id','ID','ticker','symbol', 'row','index'] if c in train_df.columns])\n",
    "meta_cols |= set([c for c in ['date','timestamp','datetime','time','Date','DATE'] if c in train_df.columns])\n",
    "\n",
    "train_numeric_cols = [c for c in train_df.select_dtypes(include=['number']).columns if c not in meta_cols]\n",
    "if _test_cols is not None:\n",
    "    # Keep only columns that will exist at prediction time\n",
    "    candidate_cols = [c for c in train_numeric_cols if c in _test_cols]\n",
    "else:\n",
    "    candidate_cols = train_numeric_cols\n",
    "\n",
    "missing_frac = train_df[candidate_cols].isna().mean()\n",
    "FEATURE_COLS = [c for c in candidate_cols if missing_frac[c] <= missing_thr]\n",
    "\n",
    "# Logging\n",
    "_dropped_missing = len(candidate_cols) - len(FEATURE_COLS)\n",
    "_dropped_absent = len(train_numeric_cols) - len(candidate_cols)\n",
    "print(f\"Selected {len(FEATURE_COLS)} features (missing ≤ {missing_thr:.0%}). Dropped { _dropped_missing } by missingness and { _dropped_absent } absent-in-test of {len(train_numeric_cols)} train numeric columns.\")\n",
    "\n",
    "# Build features/target\n",
    "X = train_df[FEATURE_COLS].copy()\n",
    "y = train_df[CONFIG['target_column']].copy()\n",
    "\n",
    "# Missing handling and consistent row filtering\n",
    "X = X.ffill().bfill()\n",
    "valid_idx = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "X = X[valid_idx]\n",
    "y = y[valid_idx]\n",
    "\n",
    "# Save training feature means for test-time imputation\n",
    "TRAIN_FEATURE_MEANS = X.mean(numeric_only=True)\n",
    "\n",
    "print(f\"Data cleaned: {len(X):,} samples; final feature count: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Series Cross-Validation Setup\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TIME-SERIES CROSS-VALIDATION SETUP\")\n",
    "print(\"=\"*70)\n",
    "tscv = TimeSeriesSplit(n_splits=CONFIG['n_splits'])\n",
    "print(f\"n_splits = {CONFIG['n_splits']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd76cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Random Forest Regressor (with volatility enforcement)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 1: RANDOM FOREST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rf_results = []\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n",
    "    print(f\"\\nFold {fold}/{CONFIG['n_splits']}\")\n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_fold)\n",
    "    X_val_scaled = scaler.transform(X_val_fold)\n",
    "\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100, max_depth=10, min_samples_split=20, min_samples_leaf=10,\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    rf_model.fit(X_train_scaled, y_train_fold)\n",
    "\n",
    "    y_pred = rf_model.predict(X_val_scaled)\n",
    "    # clip to [0,2]\n",
    "    y_pred = np.clip(y_pred, *CONFIG['prediction_bounds'])\n",
    "    # enforce volatility vs benchmark (use validation target std for fold)\n",
    "    y_pred = enforce_volatility(y_pred, y_val_fold.std(), max_ratio=CONFIG['max_volatility_ratio'], clip_bounds=CONFIG['prediction_bounds'])\n",
    "\n",
    "    mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    mae = mean_absolute_error(y_val_fold, y_pred)\n",
    "    r2 = r2_score(y_val_fold, y_pred)\n",
    "\n",
    "    rf_results.append({'fold': fold, 'mse': mse, 'mae': mae, 'r2': r2})\n",
    "    print(f\"  MSE: {mse:.6f}\")\n",
    "    print(f\"  MAE: {mae:.6f}\")\n",
    "    print(f\"  R²:  {r2:.6f}\")\n",
    "\n",
    "rf_results_df = pd.DataFrame(rf_results)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RANDOM FOREST - AVERAGE RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"MSE: {rf_results_df['mse'].mean():.6f} ± {rf_results_df['mse'].std():.6f}\")\n",
    "print(f\"MAE: {rf_results_df['mae'].mean():.6f} ± {rf_results_df['mae'].std():.6f}\")\n",
    "print(f\"R²:  {rf_results_df['r2'].mean():.6f} ± {rf_results_df['r2'].std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d98285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final Random Forest on all data\n",
    "print(\"\\nTraining final Random Forest on all data...\")\n",
    "scaler_rf = StandardScaler()\n",
    "X_scaled_all = scaler_rf.fit_transform(X)\n",
    "rf_final = RandomForestRegressor(\n",
    "    n_estimators=100, max_depth=10, min_samples_split=20, min_samples_leaf=10,\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_final.fit(X_scaled_all, y)\n",
    "joblib.dump(rf_final, CONFIG['artifacts_dir'] / 'rf_model.joblib')\n",
    "joblib.dump(scaler_rf, CONFIG['artifacts_dir'] / 'rf_scaler.joblib')\n",
    "print(\"Random Forest model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427eae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: LightGBM (with volatility enforcement)\n",
    "if LGBM_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL 2: LIGHTGBM\")\n",
    "    print(\"=\"*70)\n",
    "    lgbm_results = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n",
    "        print(f\"\\nFold {fold}/{CONFIG['n_splits']}\")\n",
    "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        lgbm_model = lgb.LGBMRegressor(\n",
    "            n_estimators=100, learning_rate=0.05, max_depth=5, num_leaves=31,\n",
    "            min_child_samples=20, subsample=0.8, colsample_bytree=0.8,\n",
    "            random_state=42, verbose=-1\n",
    "        )\n",
    "        lgbm_model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)]\n",
    "        )\n",
    "        y_pred = lgbm_model.predict(X_val_fold)\n",
    "        y_pred = np.clip(y_pred, *CONFIG['prediction_bounds'])\n",
    "        y_pred = enforce_volatility(y_pred, y_val_fold.std(), max_ratio=CONFIG['max_volatility_ratio'], clip_bounds=CONFIG['prediction_bounds'])\n",
    "        mse = mean_squared_error(y_val_fold, y_pred)\n",
    "        mae = mean_absolute_error(y_val_fold, y_pred)\n",
    "        r2 = r2_score(y_val_fold, y_pred)\n",
    "        lgbm_results.append({'fold': fold, 'mse': mse, 'mae': mae, 'r2': r2})\n",
    "        print(f\"  MSE: {mse:.6f}\")\n",
    "        print(f\"  MAE: {mae:.6f}\")\n",
    "        print(f\"  R²:  {r2:.6f}\")\n",
    "    lgbm_results_df = pd.DataFrame(lgbm_results)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"LIGHTGBM - AVERAGE RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"MSE: {lgbm_results_df['mse'].mean():.6f} ± {lgbm_results_df['mse'].std():.6f}\")\n",
    "    print(f\"MAE: {lgbm_results_df['mae'].mean():.6f} ± {lgbm_results_df['mae'].std():.6f}\")\n",
    "    print(f\"R²:  {lgbm_results_df['r2'].mean():.6f} ± {lgbm_results_df['r2'].std():.6f}\")\n",
    "else:\n",
    "    print(\"Skipping LightGBM (not installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a4603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final LightGBM on all data\n",
    "if LGBM_AVAILABLE:\n",
    "    print(\"\\nTraining final LightGBM on all data...\")\n",
    "    lgbm_final = lgb.LGBMRegressor(\n",
    "        n_estimators=100, learning_rate=0.05, max_depth=5, num_leaves=31,\n",
    "        min_child_samples=20, subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=42, verbose=-1\n",
    "    )\n",
    "    lgbm_final.fit(X, y)\n",
    "    joblib.dump(lgbm_final, CONFIG['artifacts_dir'] / 'lgbm_model.joblib')\n",
    "    print(\"LightGBM model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b6099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: PyTorch LSTM \n",
    "torch_results = []\n",
    "if TORCH_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL 3: PYTORCH LSTM\")\n",
    "    print(\"=\"*70)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    class LSTMRegressor(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super().__init__()\n",
    "            self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=50, num_layers=1, batch_first=True)\n",
    "            self.dropout1 = nn.Dropout(0.2)\n",
    "            self.lstm2 = nn.LSTM(input_size=50, hidden_size=25, num_layers=1, batch_first=True)\n",
    "            self.dropout2 = nn.Dropout(0.2)\n",
    "            self.fc = nn.Linear(25, 1)\n",
    "        def forward(self, x):\n",
    "            out, _ = self.lstm1(x)\n",
    "            out = self.dropout1(out)\n",
    "            out, _ = self.lstm2(out)\n",
    "            out = self.dropout2(out)\n",
    "            out = out[:, -1, :]\n",
    "            out = self.fc(out)\n",
    "            return out.squeeze(-1)\n",
    "\n",
    "    class EarlyStopping:\n",
    "        def __init__(self, patience=10):\n",
    "            self.patience = patience\n",
    "            self.counter = 0\n",
    "            self.best_loss = float('inf')\n",
    "            self.best_state = None\n",
    "        def step(self, val_loss, model):\n",
    "            improved = val_loss < self.best_loss - 1e-12\n",
    "            if improved:\n",
    "                self.best_loss = val_loss\n",
    "                self.counter = 0\n",
    "                self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            else:\n",
    "                self.counter += 1\n",
    "            return self.counter >= self.patience\n",
    "\n",
    "    def train_one_fold(X_train, y_train, X_val, y_val, epochs=100, batch_size=32):\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        X_train_t = torch.from_numpy(X_train_scaled.astype('float32')).unsqueeze(1)\n",
    "        X_val_t = torch.from_numpy(X_val_scaled.astype('float32')).unsqueeze(1)\n",
    "        y_train_t = torch.from_numpy(y_train.values.astype('float32'))\n",
    "        y_val_t = torch.from_numpy(y_val.values.astype('float32'))\n",
    "        train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "        val_ds = TensorDataset(X_val_t, y_val_t)\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = LSTMRegressor(input_size=X_train.shape[1]).to(device)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        early = EarlyStopping(patience=10)\n",
    "        epochs_ran = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            tr_loss = 0.0\n",
    "            for xb, yb in train_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                opt.zero_grad()\n",
    "                preds = model(xb)\n",
    "                loss = loss_fn(preds, yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                tr_loss += loss.item() * xb.size(0)\n",
    "            tr_loss /= len(train_loader.dataset)\n",
    "\n",
    "            model.eval()\n",
    "            va_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb = xb.to(device)\n",
    "                    yb = yb.to(device)\n",
    "                    preds = model(xb)\n",
    "                    loss = loss_fn(preds, yb)\n",
    "                    va_loss += loss.item() * xb.size(0)\n",
    "            va_loss /= len(val_loader.dataset)\n",
    "            epochs_ran = epoch + 1\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                print(f\"Epoch {epoch+1:3d} - train_loss={tr_loss:.6f} val_loss={va_loss:.6f}\")\n",
    "            if early.step(va_loss, model):\n",
    "                print(f\"Early stopping at epoch {epoch+1}; best val_loss={early.best_loss:.6f}\")\n",
    "                break\n",
    "        if early.best_state is not None:\n",
    "            model.load_state_dict(early.best_state)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_preds = model(X_val_t.to(device)).cpu().numpy()\n",
    "        # clip then enforce volatility w.r.t. validation target std\n",
    "        val_preds = np.clip(val_preds, *CONFIG['prediction_bounds'])\n",
    "        val_preds = enforce_volatility(val_preds, y_val.std(), max_ratio=CONFIG['max_volatility_ratio'], clip_bounds=CONFIG['prediction_bounds'])\n",
    "        mse = mean_squared_error(y_val, val_preds)\n",
    "        mae = mean_absolute_error(y_val, val_preds)\n",
    "        r2 = r2_score(y_val, val_preds)\n",
    "        return scaler, model, val_preds, mse, mae, r2, epochs_ran\n",
    "\n",
    "    torch_cv_scalers, torch_cv_models = [], []\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n",
    "        print(f\"\\nFold {fold}/{CONFIG['n_splits']}\")\n",
    "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        scaler_t, model_t, y_pred, mse, mae, r2, epochs_ran = train_one_fold(X_train_fold, y_train_fold, X_val_fold, y_val_fold)\n",
    "        torch_results.append({'fold': fold, 'mse': mse, 'mae': mae, 'r2': r2, 'epochs': epochs_ran})\n",
    "        torch_cv_scalers.append(scaler_t)\n",
    "        torch_cv_models.append(model_t)\n",
    "        print(f\"  MSE: {mse:.6f}\")\n",
    "        print(f\"  MAE: {mae:.6f}\")\n",
    "        print(f\"  R²:  {r2:.6f}\")\n",
    "        print(f\"  Epochs trained: {epochs_ran}\")\n",
    "\n",
    "    torch_results_df = pd.DataFrame(torch_results)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PYTORCH LSTM - AVERAGE RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"MSE: {torch_results_df['mse'].mean():.6f} ± {torch_results_df['mse'].std():.6f}\")\n",
    "    print(f\"MAE: {torch_results_df['mae'].mean():.6f} ± {torch_results_df['mae'].std():.6f}\")\n",
    "    print(f\"R²:  {torch_results_df['r2'].mean():.6f} ± {torch_results_df['r2'].std():.6f}\")\n",
    "else:\n",
    "    print(\"Skipping PyTorch LSTM (torch not installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d009b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final PyTorch LSTM on all data\n",
    "torch_final = None\n",
    "scaler_torch_final = None\n",
    "if TORCH_AVAILABLE:\n",
    "    print(\"\\nTraining final PyTorch LSTM on all data...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    scaler_torch_final = StandardScaler()\n",
    "    X_scaled_full = scaler_torch_final.fit_transform(X)\n",
    "    X_full_t = torch.from_numpy(X_scaled_full.astype('float32')).unsqueeze(1).to(device)\n",
    "    y_full_t = torch.from_numpy(y.values.astype('float32')).to(device)\n",
    "    ds_full = TensorDataset(X_full_t, y_full_t)\n",
    "    dl_full = DataLoader(ds_full, batch_size=32, shuffle=False)\n",
    "    model = LSTMRegressor(input_size=X.shape[1]).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    class _Early:\n",
    "        def __init__(self, patience=10):\n",
    "            self.patience = patience; self.c=0; self.best=float('inf'); self.state=None\n",
    "        def step(self, loss, model):\n",
    "            if loss < self.best - 1e-12:\n",
    "                self.best = loss; self.c=0; self.state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "            else:\n",
    "                self.c += 1\n",
    "            return self.c>=self.patience\n",
    "    early = _Early(patience=10)\n",
    "    for epoch in range(100):\n",
    "        model.train(); tr=0.0\n",
    "        for xb, yb in dl_full:\n",
    "            opt.zero_grad(); preds = model(xb); loss = loss_fn(preds, yb); loss.backward(); opt.step(); tr += loss.item()*xb.size(0)\n",
    "        tr /= len(dl_full.dataset)\n",
    "        if (epoch+1)%10==0 or epoch==0:\n",
    "            print(f\"Epoch {epoch+1:3d} - loss={tr:.6f}\")\n",
    "        if early.step(tr, model):\n",
    "            print(f\"Early stopping at epoch {epoch+1}; best loss={early.best:.6f}\")\n",
    "            break\n",
    "    if early.state is not None:\n",
    "        model.load_state_dict(early.state)\n",
    "    torch_final = model\n",
    "    torch.save(torch_final.state_dict(), CONFIG['artifacts_dir'] / 'torch_lstm_model.pt')\n",
    "    joblib.dump(scaler_torch_final, CONFIG['artifacts_dir'] / 'torch_lstm_scaler.joblib')\n",
    "    try:\n",
    "        scripted = torch.jit.script(torch_final.cpu())\n",
    "        scripted.save(str(CONFIG['artifacts_dir'] / 'torch_lstm_model_scripted.pt'))\n",
    "        torch_final.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save scripted model: {e}\")\n",
    "    print(\"PyTorch LSTM model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40487283",
   "metadata": {},
   "source": [
    "## CHOOSING BEST MODEL FOR KAGGLE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc21d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "comparison = []\n",
    "comparison.append({\n",
    "    'Model': 'Random Forest',\n",
    "    'MSE': f\"{rf_results_df['mse'].mean():.6f} ± {rf_results_df['mse'].std():.6f}\",\n",
    "    'MAE': f\"{rf_results_df['mae'].mean():.6f} ± {rf_results_df['mae'].std():.6f}\",\n",
    "    'R²':  f\"{rf_results_df['r2'].mean():.6f} ± {rf_results_df['r2'].std():.6f}\",\n",
    "})\n",
    "if LGBM_AVAILABLE:\n",
    "    comparison.append({\n",
    "        'Model': 'LightGBM',\n",
    "        'MSE': f\"{lgbm_results_df['mse'].mean():.6f} ± {lgbm_results_df['mse'].std():.6f}\",\n",
    "        'MAE': f\"{lgbm_results_df['mae'].mean():.6f} ± {lgbm_results_df['mae'].std():.6f}\",\n",
    "        'R²':  f\"{lgbm_results_df['r2'].mean():.6f} ± {lgbm_results_df['r2'].std():.6f}\",\n",
    "    })\n",
    "if TORCH_AVAILABLE and len(torch_results) > 0:\n",
    "    comparison.append({\n",
    "        'Model': 'PyTorch LSTM',\n",
    "        'MSE': f\"{torch_results_df['mse'].mean():.6f} ± {torch_results_df['mse'].std():.6f}\",\n",
    "        'MAE': f\"{torch_results_df['mae'].mean():.6f} ± {torch_results_df['mae'].std():.6f}\",\n",
    "        'R²':  f\"{torch_results_df['r2'].mean():.6f} ± {torch_results_df['r2'].std():.6f}\",\n",
    "    })\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "print(comparison_df.to_string(index=False))\n",
    "comparison_df.to_csv(CONFIG['results_dir'] / 'model_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451829f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility Analysis (post-hoc)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VOLATILITY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# RF\n",
    "X_scaled_all_rf = scaler_rf.transform(X)\n",
    "pred_rf_raw = rf_final.predict(X_scaled_all_rf)\n",
    "pred_rf_raw = np.clip(pred_rf_raw, *CONFIG['prediction_bounds'])\n",
    "benchmark_std = y.std()\n",
    "pred_rf_enf = enforce_volatility(pred_rf_raw, benchmark_std, max_ratio=CONFIG['max_volatility_ratio'], clip_bounds=CONFIG['prediction_bounds'])\n",
    "print(f\"RF - target std: {benchmark_std:.6f}, raw std: {pred_rf_raw.std():.6f}, enforced std: {np.asarray(pred_rf_enf).std():.6f}\")\n",
    "\n",
    "# Torch (if trained)\n",
    "if TORCH_AVAILABLE and 'torch_final' in globals() and torch_final is not None:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    X_scaled_all_t = scaler_torch_final.transform(X)\n",
    "    X_all_t = torch.from_numpy(X_scaled_all_t.astype('float32')).unsqueeze(1).to(device)\n",
    "    torch_final.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_t_raw = torch_final(X_all_t).detach().cpu().numpy()\n",
    "    pred_t_raw = np.clip(pred_t_raw, *CONFIG['prediction_bounds'])\n",
    "    pred_t_enf = enforce_volatility(pred_t_raw, benchmark_std, max_ratio=CONFIG['max_volatility_ratio'], clip_bounds=CONFIG['prediction_bounds'])\n",
    "    print(f\"Torch LSTM - raw std: {np.asarray(pred_t_raw).std():.6f}, enforced std: {np.asarray(pred_t_enf).std():.6f}\")\n",
    "\n",
    "print(f\"Max allowed ratio: {CONFIG['max_volatility_ratio']:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0cae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (Random Forest)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': FEATURE_COLS,\n",
    "    'importance': rf_final.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 10 MOST IMPORTANT FEATURES (Random Forest)\")\n",
    "print(\"=\"*70)\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['feature'][:10], feature_importance['importance'][:10])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 10 Feature Importance (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['results_dir'] / 'feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39049704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission using best  model\n",
    "from pathlib import Path as _Path\n",
    "\n",
    "IS_KAGGLE = _Path('/kaggle/input').exists()\n",
    "submission_path = _Path('/kaggle/working/submission.csv') if IS_KAGGLE else CONFIG['results_dir'] / 'submission.csv'\n",
    "\n",
    "test_df = pd.read_csv(CONFIG['test_data_path'])\n",
    "# Align columns with training features (add missing columns, keep order)\n",
    "X_test = test_df.reindex(columns=FEATURE_COLS, copy=True)\n",
    "# Fill missing values: within-row ffill/bfill, then fall back to training feature means\n",
    "X_test = X_test.ffill().bfill()\n",
    "if 'TRAIN_FEATURE_MEANS' in globals():\n",
    "    X_test = X_test.fillna(TRAIN_FEATURE_MEANS)\n",
    "# Final fallback for any remaining NaNs\n",
    "X_test = X_test.fillna(0.0)\n",
    "\n",
    "# Determine best model by lowest mean CV MSE\n",
    "model_scores = {'rf': float(rf_results_df['mse'].mean())}\n",
    "if 'lgbm_results_df' in globals():\n",
    "    model_scores['lgbm'] = float(lgbm_results_df['mse'].mean())\n",
    "if 'torch_results_df' in globals() and len(torch_results) > 0:\n",
    "    model_scores['torch'] = float(torch_results_df['mse'].mean())\n",
    "\n",
    "best_model = min(model_scores, key=model_scores.get)\n",
    "\n",
    "# Predict with the chosen model\n",
    "if best_model == 'torch' and TORCH_AVAILABLE and 'torch_final' in globals() and torch_final is not None and 'scaler_torch_final' in globals() and scaler_torch_final is not None:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    X_test_scaled = scaler_torch_final.transform(X_test)\n",
    "    X_test_t = torch.from_numpy(X_test_scaled.astype('float32')).unsqueeze(1).to(device)\n",
    "    torch_final.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = torch_final(X_test_t).detach().cpu().numpy().reshape(-1)\n",
    "elif best_model == 'lgbm' and 'lgbm_final' in globals():\n",
    "    preds = lgbm_final.predict(X_test)\n",
    "else:\n",
    "    X_test_scaled = scaler_rf.transform(X_test)\n",
    "    preds = rf_final.predict(X_test_scaled)\n",
    "\n",
    "# Clip and enforce volatility\n",
    "preds = np.clip(preds, *CONFIG['prediction_bounds'])\n",
    "preds = enforce_volatility(preds, y.std(), max_ratio=CONFIG['max_volatility_ratio'], clip_bounds=CONFIG['prediction_bounds'])\n",
    "\n",
    "# Build submission\n",
    "sample_sub_path = _Path('/kaggle/input/hull-tactical-market-prediction/sample_submission.csv')\n",
    "if sample_sub_path.exists():\n",
    "    sample_sub = pd.read_csv(sample_sub_path)\n",
    "    id_col = sample_sub.columns[0]\n",
    "    if id_col in test_df.columns:\n",
    "        submission_df = pd.DataFrame({id_col: test_df[id_col], 'prediction': preds})\n",
    "    else:\n",
    "        submission_df = pd.DataFrame({id_col: range(len(test_df)), 'prediction': preds})\n",
    "elif 'id' in test_df.columns:\n",
    "    submission_df = pd.DataFrame({'id': test_df['id'], 'prediction': preds})\n",
    "else:\n",
    "    submission_df = pd.DataFrame({'row': np.arange(len(test_df)), 'prediction': preds})\n",
    "\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"Saved submission ({best_model}) -> {submission_path}\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c56d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final confirmation and Kaggle output declaration\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NOTEBOOK EXECUTION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "kaggle_submission = Path('/kaggle/working/submission.csv')\n",
    "local_submission = CONFIG['results_dir'] / 'submission.csv'\n",
    "\n",
    "if kaggle_submission.exists():\n",
    "    submission_file = kaggle_submission\n",
    "    print(f\"✓ Submission file found at: {submission_file}\")\n",
    "elif local_submission.exists():\n",
    "    submission_file = local_submission\n",
    "    print(f\"WARNING: Submission file at wrong location: {submission_file}\")\n",
    "    if Path('/kaggle/working').exists():\n",
    "        import shutil\n",
    "        shutil.copy(submission_file, kaggle_submission)\n",
    "        submission_file = kaggle_submission\n",
    "        print(f\"✓ Copied to correct location: {submission_file}\")\n",
    "else:\n",
    "    print(f\"✗ ERROR: Submission file not found at either location:\")\n",
    "    print(f\"  - {kaggle_submission}\")\n",
    "    print(f\"  - {local_submission}\")\n",
    "    raise FileNotFoundError(\"Submission file not created!\")\n",
    "\n",
    "print(f\"Size: {submission_file.stat().st_size:,} bytes\")\n",
    "sub_df = pd.read_csv(submission_file)\n",
    "print(f\"Shape: {sub_df.shape}\")\n",
    "print(f\"Columns: {list(sub_df.columns)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(sub_df.head())\n",
    "if sub_df.shape[0] == 0:\n",
    "    raise ValueError(\"Submission file is empty!\")\n",
    "if sub_df.isnull().any().any():\n",
    "    raise ValueError(\"Submission contains null values!\")\n",
    "print(\"\\n✓ Submission file validated successfully\")\n",
    "print(\"✓ Ready for Kaggle competition submission\")\n",
    "print(f\"\\n** SUBMISSION FILE LOCATION: {submission_file.absolute()} **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca56b5c3",
   "metadata": {},
   "source": [
    "## THIS IS STUFF TO MAKE SUBMISSION WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0623c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference server: predict() and gateway wiring\n",
    "import os\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Polars may be provided by the competition environment\n",
    "try:\n",
    "    import polars as pl  # type: ignore\n",
    "    _POLARS_AVAILABLE = True\n",
    "except Exception:\n",
    "    pl = None  # type: ignore\n",
    "    _POLARS_AVAILABLE = False\n",
    "\n",
    "# Reuse availability flags from earlier if present; else recompute minimally\n",
    "try:\n",
    "    LGBM_AVAILABLE  # type: ignore[name-defined]\n",
    "except NameError:\n",
    "    LGBM_AVAILABLE = importlib.util.find_spec(\"lightgbm\") is not None\n",
    "if LGBM_AVAILABLE:\n",
    "    import lightgbm as lgb  # type: ignore\n",
    "\n",
    "# Reuse helper from earlier cells if present; else define a minimal one\n",
    "if \"enforce_volatility\" not in globals():\n",
    "    def enforce_volatility(preds, target_std, max_ratio=1.2, clip_bounds=(0.0, 2.0)):\n",
    "        arr = np.asarray(preds, dtype=float).reshape(-1)\n",
    "        pred_std = float(arr.std())\n",
    "        tgt_std = float(target_std)\n",
    "        if pred_std > 0 and tgt_std > 0:\n",
    "            scale = min(1.0, (max_ratio * tgt_std) / (pred_std + 1e-12))\n",
    "            mean = float(arr.mean())\n",
    "            arr = mean + scale * (arr - mean)\n",
    "        if clip_bounds is not None:\n",
    "            arr = np.clip(arr, clip_bounds[0], clip_bounds[1])\n",
    "        return arr\n",
    "\n",
    "# Threshold from CONFIG if present, else use global constant\n",
    "try:\n",
    "    _MISSING_THR = float(CONFIG.get('missing_threshold', MISSING_DATA_THRESHOLD))  # type: ignore[name-defined]\n",
    "except Exception:\n",
    "    _MISSING_THR = 0.10  # Default to 10% if neither CONFIG nor constant available\n",
    "\n",
    "# Competition paths\n",
    "COMP_ROOT = Path(\"/kaggle/input/hull-tactical-market-prediction\")\n",
    "TRAIN_CSV = COMP_ROOT / \"train.csv\"  # published train data\n",
    "\n",
    "# Globals managed by inference\n",
    "_INF_READY = False\n",
    "_INF_MODEL = None\n",
    "_INF_BENCH_STD = None  # std of training targets\n",
    "_INF_FEATURE_COLS = None  # feature list determined from training data by missingness\n",
    "_INF_TRAIN_MEANS = None   # per-feature means from training data\n",
    "\n",
    "\n",
    "def _infer_feature_cols(df: pd.DataFrame, target_col: str, missing_thr: float) -> list[str]:\n",
    "    meta_cols = set([target_col])\n",
    "    meta_cols |= set([c for c in ['id','ID','ticker','symbol','row','index'] if c in df.columns])\n",
    "    meta_cols |= set([c for c in ['date','timestamp','datetime','time','Date','DATE'] if c in df.columns])\n",
    "    numeric_cols = [c for c in df.select_dtypes(include=['number']).columns if c not in meta_cols]\n",
    "    miss = df[numeric_cols].isna().mean()\n",
    "    return [c for c in numeric_cols if miss[c] <= missing_thr]\n",
    "\n",
    "\n",
    "def _safe_prepare_frame(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Select required features, forward/backward fill missing, fallback to train means, return DataFrame.\"\"\"\n",
    "    global _INF_FEATURE_COLS, _INF_TRAIN_MEANS\n",
    "    if _INF_FEATURE_COLS is None:\n",
    "        # Fall back to any globally defined FEATURE_COLS\n",
    "        if 'FEATURE_COLS' in globals() and isinstance(FEATURE_COLS, (list, tuple)) and len(FEATURE_COLS) > 0:\n",
    "            _INF_FEATURE_COLS = list(FEATURE_COLS)\n",
    "        else:\n",
    "            _INF_FEATURE_COLS = _infer_feature_cols(df_in, 'forward_returns', _MISSING_THR)\n",
    "    X_batch = df_in.reindex(columns=_INF_FEATURE_COLS, copy=True)\n",
    "    X_batch = X_batch.ffill().bfill()\n",
    "    if _INF_TRAIN_MEANS is not None:\n",
    "        X_batch = X_batch.fillna(_INF_TRAIN_MEANS)\n",
    "    X_batch = X_batch.fillna(0.0)\n",
    "    return X_batch\n",
    "\n",
    "\n",
    "def _init_inference():\n",
    "    \"\"\"Train a fast model on published training data on first call.\"\"\"\n",
    "    global _INF_READY, _INF_MODEL, _INF_BENCH_STD, _INF_FEATURE_COLS, _INF_TRAIN_MEANS\n",
    "\n",
    "    if _INF_READY:\n",
    "        return\n",
    "\n",
    "    if not TRAIN_CSV.exists():\n",
    "        raise FileNotFoundError(f\"Training data not found at {TRAIN_CSV}\")\n",
    "\n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    target_col = 'forward_returns'\n",
    "    if target_col not in train_df.columns:\n",
    "        raise ValueError(\"Target column 'forward_returns' not found in training data.\")\n",
    "\n",
    "    # Determine features once using missingness\n",
    "    _INF_FEATURE_COLS = _infer_feature_cols(train_df, target_col, _MISSING_THR)\n",
    "\n",
    "    y = train_df[target_col]\n",
    "    X = train_df.reindex(columns=_INF_FEATURE_COLS, copy=True)\n",
    "    X = X.ffill().bfill()\n",
    "    # Compute and store train means for robust imputation later\n",
    "    _INF_TRAIN_MEANS = X.mean(numeric_only=True)\n",
    "    X = X.fillna(_INF_TRAIN_MEANS).fillna(0.0)\n",
    "\n",
    "    _INF_BENCH_STD = float(pd.Series(y).std())\n",
    "\n",
    "    # Prefer LightGBM for speed\n",
    "    if LGBM_AVAILABLE:\n",
    "        model = lgb.LGBMRegressor(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            num_leaves=31,\n",
    "            min_child_samples=20,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "        _INF_MODEL = model\n",
    "    else:\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=12,\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "        _INF_MODEL = model\n",
    "\n",
    "    _INF_READY = True\n",
    "\n",
    "\n",
    "def _to_pandas(df_any) -> pd.DataFrame:\n",
    "    if _POLARS_AVAILABLE and isinstance(df_any, pl.DataFrame):  # type: ignore\n",
    "        return df_any.to_pandas()\n",
    "    if isinstance(df_any, pd.DataFrame):\n",
    "        return df_any\n",
    "    return pd.DataFrame(df_any)\n",
    "\n",
    "\n",
    "def _predict_batch(df_any) -> np.ndarray:\n",
    "    if not _INF_READY:\n",
    "        _init_inference()\n",
    "    dfp = _to_pandas(df_any)\n",
    "    Xb = _safe_prepare_frame(dfp)\n",
    "    preds = _INF_MODEL.predict(Xb)  # type: ignore[union-attr]\n",
    "    preds = np.clip(preds, *(0.0, 2.0))\n",
    "    preds = enforce_volatility(preds, _INF_BENCH_STD, max_ratio=1.2, clip_bounds=(0.0, 2.0))\n",
    "    return np.asarray(preds, dtype=float).reshape(-1)\n",
    "\n",
    "\n",
    "def predict(test) -> float:\n",
    "    preds = _predict_batch(test)\n",
    "    if preds.shape[0] == 1:\n",
    "        return float(preds[0])\n",
    "    if _POLARS_AVAILABLE:\n",
    "        return pl.DataFrame({\"prediction\": preds})  # type: ignore\n",
    "    return pd.DataFrame({\"prediction\": preds})\n",
    "\n",
    "# Start the evaluation server (remote) or a local gateway (for local testing)\n",
    "import kaggle_evaluation.default_inference_server as _kserv\n",
    "_inference_server = _kserv.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    _inference_server.serve()\n",
    "else:\n",
    "    _inference_server.run_local_gateway((str(COMP_ROOT),))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
